{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65fbe2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Dataset for instruction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed518003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.16.2-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.41-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.40.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (65.6.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-4.25.2-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.16.2-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.2 MB 1.4 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.1/2.2 MB 1.3 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.4/2.2 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 5.1 MB/s eta 0:00:00\n",
      "Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
      "   ---------------------------------------- 0.0/196.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 196.4/196.4 kB 11.6 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.2-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.4/413.4 kB 13.0 MB/s eta 0:00:00\n",
      "Downloading sentry_sdk-1.40.0-py2.py3-none-any.whl (257 kB)\n",
      "   ---------------------------------------- 0.0/257.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 257.5/257.5 kB 16.5 MB/s eta 0:00:00\n",
      "Downloading setproctitle-1.3.3-cp310-cp310-win_amd64.whl (11 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.41 docker-pycreds-0.4.0 gitdb-4.0.11 protobuf-4.25.2 sentry-sdk-1.40.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts wandb.exe and wb.exe are installed in 'C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5070adad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutshavpaudel46\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c02eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Loading data\n",
    "import json\n",
    "with open(\"alpaca_gpt4_data.json\", \"r\") as f:\n",
    "    alpaca = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fcd40ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alpaca)                                         #52 k alpaca datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a88437b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\ASUS\\Desktop\\LLM zero to hero\\Day15\\wandb\\run-20240205_005833-wk3x5fjh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/utshavpaudel46/alpaca_finetuning/runs/wk3x5fjh' target=\"_blank\">kind-water-4</a></strong> to <a href='https://wandb.ai/utshavpaudel46/alpaca_finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/utshavpaudel46/alpaca_finetuning' target=\"_blank\">https://wandb.ai/utshavpaudel46/alpaca_finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/utshavpaudel46/alpaca_finetuning/runs/wk3x5fjh' target=\"_blank\">https://wandb.ai/utshavpaudel46/alpaca_finetuning/runs/wk3x5fjh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.023 MB uploaded\\r'), FloatProgress(value=0.049806082874055926, max=1â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">kind-water-4</strong> at: <a href='https://wandb.ai/utshavpaudel46/alpaca_finetuning/runs/wk3x5fjh' target=\"_blank\">https://wandb.ai/utshavpaudel46/alpaca_finetuning/runs/wk3x5fjh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240205_005833-wk3x5fjh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"alpaca_finetuning\"):                                       # initializing login\n",
    "    at = wandb.Artifact(                                                            # creating artifact with details\n",
    "        name=\"alpaca_gpt4\",\n",
    "        type=\"dataset\",\n",
    "        description=\"A gpt4 generated alpaca like dataset for instruction finetuning\",\n",
    "        metadata = {\"url\":\"https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data\"},\n",
    "        )\n",
    "    at.add_file(\"alpaca_gpt4_data.json\")                                           # adding files to artifact\n",
    "    \n",
    "    # table\n",
    "    table = wandb.Table(columns=list(alpaca[0].keys()))                            # turning data into table form\n",
    "    for row in alpaca:                                                             # adding values in table\n",
    "        table.add_data(*row.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39833c7f",
   "metadata": {},
   "source": [
    "# Dataset prepartion and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f1b958e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Sort the following list in alphabetical order.',\n",
       " 'input': 'Camouflage, Furniture, Plaster',\n",
       " 'output': 'Camouflage, Furniture, Plaster sorted in alphabetical order:\\nCamouflage, Furniture, Plaster'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting datasets\n",
    "alpaca[232]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e031999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing functions for fromatting datasets to feed LLM\n",
    "def prompt_no_input(row):\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\".format_map(row)\n",
    ")\n",
    "def prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\".format_map(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1758d942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Sort the following list in alphabetical order.', 'input': 'Camouflage, Furniture, Plaster', 'output': 'Camouflage, Furniture, Plaster sorted in alphabetical order:\\nCamouflage, Furniture, Plaster'}\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Camouflage, Furniture, Plaster\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets check the preprocessing fucntions\n",
    "row = alpaca[232]\n",
    "\n",
    "print(row)\n",
    "print(prompt_input(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5895656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging both functions\n",
    "def create_prompt(row):\n",
    "    return(prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row))\n",
    "    \n",
    "prompts = [create_prompt(row) for row in alpaca]                         # all llm inputs are here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c8aef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding EOS tokens so the model know when to stop producing text\n",
    "EOS = \"</s>\"                                                             # for llama model\n",
    "outputs = [row['output'] + EOS for row in alpaca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac95c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observing the eos tokens is added or not\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7a9ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing final dataset\n",
    "dataset = [{\"prompts\":p, \"output\":o, \"example\": p+o} for p,o in zip(prompts, outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18e43db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompts': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n',\n",
       " 'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>',\n",
       " 'example': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1219b045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\ASUS\\Desktop\\LLM zero to hero\\Day15\\wandb\\run-20240205_005855-0gadnknc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/utshavpaudel46/alpaca_finetuning/runs/0gadnknc' target=\"_blank\">usual-dream-5</a></strong> to <a href='https://wandb.ai/utshavpaudel46/alpaca_finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/utshavpaudel46/alpaca_finetuning' target=\"_blank\">https://wandb.ai/utshavpaudel46/alpaca_finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/utshavpaudel46/alpaca_finetuning/runs/0gadnknc' target=\"_blank\">https://wandb.ai/utshavpaudel46/alpaca_finetuning/runs/0gadnknc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='7.384 MB of 114.471 MB uploaded\\r'), FloatProgress(value=0.06450538370829999, max=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">usual-dream-5</strong> at: <a href='https://wandb.ai/utshavpaudel46/alpaca_finetuning/runs/0gadnknc' target=\"_blank\">https://wandb.ai/utshavpaudel46/alpaca_finetuning/runs/0gadnknc</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240205_005855-0gadnknc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating train-eval split\n",
    "import random\n",
    "import pandas as pd\n",
    "random.shuffle(dataset)                              # suffling dataset\n",
    "\n",
    "train_dataset = dataset[:-1000]                      # creating trainset\n",
    "eval_dataset = dataset[-1000:]                       # creating valset\n",
    "\n",
    "train_table = wandb.Table(dataframe=pd.DataFrame(train_dataset))       # converting dataset to table format\n",
    "eval_table = wandb.Table(dataframe=pd.DataFrame(eval_dataset))\n",
    "\n",
    "with wandb.init(project=\"alpaca_finetuning\", job_type=\"split_data\"):   # initializing on wandb \n",
    "    wandb.log({\"train_dataset\":train_table, \"eval_dataset\":eval_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e0be27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e2cad74b5242a38501bd0c0e4ce4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a004981b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0279a69de82545348beda08dea95205d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-2-7b-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad0182b59d64d959c98549d848cd7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e881c7deae248699b75d547c62abd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e25c34bf0a49baadef6f157af1ff1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id = 'meta-llama/Llama-2-7b-hf' \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token='')   # enter your hugging face token\n",
    "tokenizer.pad_token = tokenizer.eos_token                       # so that model thinks padding as end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10e51090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ packing dataset i.e example row to length of 1024 so training become efficients\n",
    "\n",
    "max_seq_len = 1024\n",
    "\n",
    "\n",
    "def pack(dataset, max_seq_len=1024):\n",
    "    tkds_ids = tokenizer([s[\"example\"] for s in dataset])[\"input_ids\"]           # return list of input ids by tokenizing example i.e instruction + outptus\n",
    "    \n",
    "    all_token_ids = []\n",
    "    for tokenized_input in tkds_ids:\n",
    "        all_token_ids.extend(tokenized_input + [tokenizer.eos_token_id])         # puts the eos after every example sequence\n",
    "    \n",
    "    packed_ds = []\n",
    "    for i in range(0, len(all_token_ids), max_seq_len+1):                        \n",
    "        input_ids = all_token_ids[i : i + max_seq_len+1]                        # packs the tokne equals to window size = 1024\n",
    "        if len(input_ids) == (max_seq_len+1):\n",
    "            packed_ds.append({\"input_ids\": input_ids[:-1], \"labels\": input_ids[1:]})  \n",
    "    return packed_ds\n",
    "\n",
    "\n",
    "train_ds_packed = pack(train_dataset)\n",
    "eval_ds_packed = pack(eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0536a93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\ASUS\\Desktop\\LLM zero to hero\\Day15\\wandb\\run-20240205_010805-djivod4z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/utshavpaudel46/alpaca_ft/runs/djivod4z' target=\"_blank\">eternal-pyramid-1</a></strong> to <a href='https://wandb.ai/utshavpaudel46/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/utshavpaudel46/alpaca_ft' target=\"_blank\">https://wandb.ai/utshavpaudel46/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/utshavpaudel46/alpaca_ft/runs/djivod4z' target=\"_blank\">https://wandb.ai/utshavpaudel46/alpaca_ft/runs/djivod4z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 130.961 MB uploaded\\r'), FloatProgress(value=8.89149343540894e-06, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eternal-pyramid-1</strong> at: <a href='https://wandb.ai/utshavpaudel46/alpaca_ft/runs/djivod4z' target=\"_blank\">https://wandb.ai/utshavpaudel46/alpaca_ft/runs/djivod4z</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240205_010805-djivod4z\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@ storing the packed datasets\n",
    "import json\n",
    "def save_jsonl(data, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for entry in data:\n",
    "            json.dump(entry, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "\n",
    "# dump everything to jsonl files\n",
    "save_jsonl(train_ds_packed, \"train_packed_alpaca.jsonl\")\n",
    "save_jsonl(eval_ds_packed, \"eval_packed_alpaca.jsonl\")\n",
    "\n",
    "\n",
    "# Create a W&B artifact\n",
    "packed_at = wandb.Artifact(\n",
    "    name=\"packed_alpaca\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Alpaca dataset packed in sequences\",\n",
    "    metadata={\"max_seq_len\":1024, \"model_id\":model_id})\n",
    "\n",
    "\n",
    "packed_at.add_file(\"train_packed_alpaca.jsonl\")\n",
    "packed_at.add_file(\"eval_packed_alpaca.jsonl\")\n",
    "\n",
    "\n",
    "# log the artifact to the project, we can give this run a job_type like `preprocess`\n",
    "with wandb.init(project=\"alpaca_ft\", job_type=\"preprocess\"):\n",
    "    wandb.log_artifact(packed_at)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53833a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
